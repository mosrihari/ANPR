{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-01T00:36:56.819759Z","iopub.execute_input":"2021-06-01T00:36:56.820186Z","iopub.status.idle":"2021-06-01T00:36:56.826938Z","shell.execute_reply.started":"2021-06-01T00:36:56.820100Z","shell.execute_reply":"2021-06-01T00:36:56.825709Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!conda install -y gdown","metadata":{"execution":{"iopub.status.busy":"2021-06-01T00:37:39.754657Z","iopub.execute_input":"2021-06-01T00:37:39.755064Z","iopub.status.idle":"2021-06-01T00:38:58.607558Z","shell.execute_reply.started":"2021-06-01T00:37:39.755029Z","shell.execute_reply":"2021-06-01T00:38:58.606215Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - gdown\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    ca-certificates-2021.5.30  |       ha878542_0         136 KB  conda-forge\n    certifi-2021.5.30          |   py37h89c1867_0         141 KB  conda-forge\n    conda-4.10.1               |   py37h89c1867_0         3.1 MB  conda-forge\n    filelock-3.0.12            |     pyh9f0ad1d_0          10 KB  conda-forge\n    gdown-3.13.0               |     pyhd8ed1ab_0          12 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         3.3 MB\n\nThe following NEW packages will be INSTALLED:\n\n  filelock           conda-forge/noarch::filelock-3.0.12-pyh9f0ad1d_0\n  gdown              conda-forge/noarch::gdown-3.13.0-pyhd8ed1ab_0\n\nThe following packages will be UPDATED:\n\n  ca-certificates                      2020.12.5-ha878542_0 --> 2021.5.30-ha878542_0\n  certifi                          2020.12.5-py37h89c1867_1 --> 2021.5.30-py37h89c1867_0\n  conda                                4.9.2-py37h89c1867_0 --> 4.10.1-py37h89c1867_0\n\n\n\nDownloading and Extracting Packages\nca-certificates-2021 | 136 KB    | ##################################### | 100% \nfilelock-3.0.12      | 10 KB     | ##################################### | 100% \nconda-4.10.1         | 3.1 MB    | ##################################### | 100% \ncertifi-2021.5.30    | 141 KB    | ##################################### | 100% \ngdown-3.13.0         | 12 KB     | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown https://drive.google.com/drive/folders/1Yio34S3msz33Vdf-01WNZlUbZaE8dQci?usp=sharing","metadata":{"execution":{"iopub.status.busy":"2021-06-01T00:42:09.227597Z","iopub.execute_input":"2021-06-01T00:42:09.227970Z","iopub.status.idle":"2021-06-01T00:42:11.136217Z","shell.execute_reply.started":"2021-06-01T00:42:09.227936Z","shell.execute_reply":"2021-06-01T00:42:11.134607Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.7/site-packages/gdown/parse_url.py:31: UserWarning: You specified Google Drive Link but it is not the correct link to download the file. Maybe you should try: https://drive.google.com/uc?id=None\n  url=\"https://drive.google.com/uc?id={}\".format(file_id)\nDownloading...\nFrom: https://drive.google.com/drive/folders/1Yio34S3msz33Vdf-01WNZlUbZaE8dQci?usp=sharing\nTo: /kaggle/working/1Yio34S3msz33Vdf-01WNZlUbZaE8dQci?usp=sharing\n222kB [00:00, 1.12MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.applications import resnet50\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_model(output_shape, lr=0.0001, training=False):\n    base_model = resnet50.ResNet50(weights=\"imagenet\",include_top=False,input_tensor=Input(shape=(80,80,3)))\n    headModel = base_model.output\n    headModel = AveragePooling2D(pool_size=(3, 3))(headModel)\n    headModel = Flatten(name=\"flatten\")(headModel)\n    headModel = Dense(128, activation=\"relu\")(headModel)\n    headModel = Dropout(0.5)(headModel)\n    headModel = Dense(output_shape, activation=\"softmax\")(headModel)\n    \n    model = Model(inputs=base_model.input, outputs=headModel)\n    # Training params\n    if training:\n        for layer in base_model.layers:\n            layer.trainable = True\n        optimizer = Adam(lr=lr)\n        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,metrics=[\"accuracy\"])    \n        \n    return model\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1./255,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,\n                                   validation_split=0.2)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\nDIRECTORY = \"../input/dataset-recog/dataset_characters\"\ntrain_datagen_dir = train_datagen.flow_from_directory(directory=DIRECTORY,target_size=(80,80), batch_size=32,\n                                                      subset=\"training\")\n\nvalidation_datagen_dir = train_datagen.flow_from_directory(directory=DIRECTORY,target_size=(80,80), batch_size=32,\n                                                      subset=\"validation\")\nmodel = create_model(output_shape=36, training=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T00:49:13.102821Z","iopub.execute_input":"2021-06-01T00:49:13.103205Z","iopub.status.idle":"2021-06-01T00:49:18.141031Z","shell.execute_reply.started":"2021-06-01T00:49:13.103173Z","shell.execute_reply":"2021-06-01T00:49:18.139801Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Found 30110 images belonging to 36 classes.\nFound 7513 images belonging to 36 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit_generator(train_datagen_dir,epochs=30,validation_data=validation_datagen_dir)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T00:49:18.143335Z","iopub.execute_input":"2021-06-01T00:49:18.144263Z","iopub.status.idle":"2021-06-01T01:50:05.922584Z","shell.execute_reply.started":"2021-06-01T00:49:18.144188Z","shell.execute_reply":"2021-06-01T01:50:05.921520Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1/30\n941/941 [==============================] - 189s 193ms/step - loss: 1.6232 - accuracy: 0.5869 - val_loss: 0.7634 - val_accuracy: 0.8131\nEpoch 2/30\n941/941 [==============================] - 120s 128ms/step - loss: 0.3184 - accuracy: 0.9186 - val_loss: 0.5377 - val_accuracy: 0.8807\nEpoch 3/30\n941/941 [==============================] - 120s 127ms/step - loss: 0.2307 - accuracy: 0.9385 - val_loss: 0.8345 - val_accuracy: 0.8662\nEpoch 4/30\n941/941 [==============================] - 119s 126ms/step - loss: 0.1814 - accuracy: 0.9508 - val_loss: 0.5809 - val_accuracy: 0.8927\nEpoch 5/30\n941/941 [==============================] - 120s 127ms/step - loss: 0.1748 - accuracy: 0.9512 - val_loss: 0.5630 - val_accuracy: 0.8915\nEpoch 6/30\n941/941 [==============================] - 119s 127ms/step - loss: 0.1539 - accuracy: 0.9550 - val_loss: 0.6660 - val_accuracy: 0.8861\nEpoch 7/30\n941/941 [==============================] - 120s 127ms/step - loss: 0.1354 - accuracy: 0.9606 - val_loss: 0.9153 - val_accuracy: 0.8865\nEpoch 8/30\n941/941 [==============================] - 120s 128ms/step - loss: 0.1192 - accuracy: 0.9649 - val_loss: 0.5133 - val_accuracy: 0.8978\nEpoch 9/30\n941/941 [==============================] - 119s 126ms/step - loss: 0.1053 - accuracy: 0.9691 - val_loss: 0.6275 - val_accuracy: 0.8959\nEpoch 10/30\n941/941 [==============================] - 121s 129ms/step - loss: 0.0998 - accuracy: 0.9696 - val_loss: 0.6150 - val_accuracy: 0.8855\nEpoch 11/30\n941/941 [==============================] - 119s 126ms/step - loss: 0.0932 - accuracy: 0.9714 - val_loss: 0.6003 - val_accuracy: 0.8895\nEpoch 12/30\n941/941 [==============================] - 118s 126ms/step - loss: 0.0935 - accuracy: 0.9727 - val_loss: 0.4335 - val_accuracy: 0.9090\nEpoch 13/30\n941/941 [==============================] - 119s 127ms/step - loss: 0.0766 - accuracy: 0.9755 - val_loss: 0.7140 - val_accuracy: 0.8895\nEpoch 14/30\n941/941 [==============================] - 118s 125ms/step - loss: 0.0685 - accuracy: 0.9776 - val_loss: 0.5852 - val_accuracy: 0.8959\nEpoch 15/30\n941/941 [==============================] - 118s 126ms/step - loss: 0.0608 - accuracy: 0.9804 - val_loss: 0.7182 - val_accuracy: 0.8937\nEpoch 16/30\n941/941 [==============================] - 119s 127ms/step - loss: 0.0708 - accuracy: 0.9784 - val_loss: 0.7972 - val_accuracy: 0.8915\nEpoch 17/30\n941/941 [==============================] - 119s 127ms/step - loss: 0.0660 - accuracy: 0.9802 - val_loss: 1.0704 - val_accuracy: 0.8849\nEpoch 18/30\n941/941 [==============================] - 119s 127ms/step - loss: 0.0602 - accuracy: 0.9798 - val_loss: 0.9231 - val_accuracy: 0.8966\nEpoch 19/30\n941/941 [==============================] - 119s 126ms/step - loss: 0.0576 - accuracy: 0.9816 - val_loss: 1.4698 - val_accuracy: 0.8736\nEpoch 20/30\n941/941 [==============================] - 119s 126ms/step - loss: 0.0600 - accuracy: 0.9810 - val_loss: 0.7232 - val_accuracy: 0.9063\nEpoch 21/30\n941/941 [==============================] - 119s 127ms/step - loss: 0.0600 - accuracy: 0.9809 - val_loss: 0.8843 - val_accuracy: 0.8890\nEpoch 22/30\n941/941 [==============================] - 120s 128ms/step - loss: 0.0535 - accuracy: 0.9830 - val_loss: 0.6527 - val_accuracy: 0.8975\nEpoch 23/30\n941/941 [==============================] - 120s 128ms/step - loss: 0.0517 - accuracy: 0.9833 - val_loss: 0.7901 - val_accuracy: 0.8883\nEpoch 24/30\n941/941 [==============================] - 120s 127ms/step - loss: 0.0464 - accuracy: 0.9858 - val_loss: 0.5122 - val_accuracy: 0.9011\nEpoch 25/30\n941/941 [==============================] - 119s 127ms/step - loss: 0.0522 - accuracy: 0.9824 - val_loss: 0.8819 - val_accuracy: 0.8887\nEpoch 26/30\n941/941 [==============================] - 120s 127ms/step - loss: 0.0487 - accuracy: 0.9834 - val_loss: 0.9536 - val_accuracy: 0.8976\nEpoch 27/30\n941/941 [==============================] - 119s 127ms/step - loss: 0.0480 - accuracy: 0.9843 - val_loss: 0.6020 - val_accuracy: 0.9008\nEpoch 28/30\n941/941 [==============================] - 119s 126ms/step - loss: 0.0474 - accuracy: 0.9856 - val_loss: 0.6561 - val_accuracy: 0.8970\nEpoch 29/30\n941/941 [==============================] - 119s 127ms/step - loss: 0.0470 - accuracy: 0.9842 - val_loss: 0.7187 - val_accuracy: 0.8987\nEpoch 30/30\n941/941 [==============================] - 118s 125ms/step - loss: 0.0429 - accuracy: 0.9870 - val_loss: 0.9442 - val_accuracy: 0.8871\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f45b042a610>"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.models import save_model\n\nsave_model(model, \"dataset_recog_resnet.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-06-01T01:54:05.878052Z","iopub.execute_input":"2021-06-01T01:54:05.878449Z","iopub.status.idle":"2021-06-01T01:54:06.960427Z","shell.execute_reply.started":"2021-06-01T01:54:05.878417Z","shell.execute_reply":"2021-06-01T01:54:06.959334Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Mobilenet","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.models import model_from_json\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport glob\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-06-01T02:54:30.244217Z","iopub.execute_input":"2021-06-01T02:54:30.244584Z","iopub.status.idle":"2021-06-01T02:54:36.794805Z","shell.execute_reply.started":"2021-06-01T02:54:30.244503Z","shell.execute_reply":"2021-06-01T02:54:36.793984Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"dataset_paths = glob.glob(\"../input/dataset-recog/dataset_characters/**/*.jpg\")","metadata":{"execution":{"iopub.status.busy":"2021-06-01T02:55:23.020565Z","iopub.execute_input":"2021-06-01T02:55:23.020934Z","iopub.status.idle":"2021-06-01T02:55:26.411294Z","shell.execute_reply.started":"2021-06-01T02:55:23.020902Z","shell.execute_reply":"2021-06-01T02:55:26.410390Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nX=[]\nlabels=[]\n\nfor image_path in dataset_paths:\n  label = image_path.split(os.path.sep)[-2]\n  image=load_img(image_path,target_size=(80,80))\n  image=img_to_array(image)\n\n  X.append(image)\n  labels.append(label)\n\nX = np.array(X,dtype=\"float16\")\nlabels = np.array(labels)\n\nprint(\"[INFO] Find {:d} images with {:d} classes\".format(len(X),len(set(labels))))\n\n\n# perform one-hot encoding on the labels\nlb = LabelEncoder()\nlb.fit(labels)\nlabels = lb.transform(labels)\ny = to_categorical(labels)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T02:56:01.506298Z","iopub.execute_input":"2021-06-01T02:56:01.506639Z","iopub.status.idle":"2021-06-01T02:57:37.738197Z","shell.execute_reply.started":"2021-06-01T02:56:01.506606Z","shell.execute_reply":"2021-06-01T02:57:37.737300Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[INFO] Find 37623 images with 36 classes\n","output_type":"stream"}]},{"cell_type":"code","source":"(trainX, testX, trainY, testY) = train_test_split(X, y, test_size=0.10, stratify=y, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T02:57:37.739635Z","iopub.execute_input":"2021-06-01T02:57:37.740134Z","iopub.status.idle":"2021-06-01T02:57:39.383871Z","shell.execute_reply.started":"2021-06-01T02:57:37.740096Z","shell.execute_reply":"2021-06-01T02:57:39.382684Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"image_gen = ImageDataGenerator(rotation_range=10,\n                              width_shift_range=0.1,\n                              height_shift_range=0.1,\n                              shear_range=0.1,\n                              zoom_range=0.1,\n                              fill_mode=\"nearest\"\n                              )","metadata":{"execution":{"iopub.status.busy":"2021-06-01T02:57:51.190865Z","iopub.execute_input":"2021-06-01T02:57:51.191193Z","iopub.status.idle":"2021-06-01T02:57:51.197130Z","shell.execute_reply.started":"2021-06-01T02:57:51.191164Z","shell.execute_reply":"2021-06-01T02:57:51.196164Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def create_model(lr=1e-4,decay=1e-4/25, training=False,output_shape=y.shape[1]):\n    baseModel = MobileNetV2(weights=\"imagenet\", \n                            include_top=False,\n                            input_tensor=Input(shape=(80, 80, 3)))\n\n    headModel = baseModel.output\n    headModel = AveragePooling2D(pool_size=(3, 3))(headModel)\n    headModel = Flatten(name=\"flatten\")(headModel)\n    headModel = Dense(128, activation=\"relu\")(headModel)\n    headModel = Dropout(0.5)(headModel)\n    headModel = Dense(output_shape, activation=\"softmax\")(headModel)\n    \n    model = Model(inputs=baseModel.input, outputs=headModel)\n    \n    if training:\n        # define trainable lalyer\n        for layer in baseModel.layers:\n            layer.trainable = True\n        # compile model\n        optimizer = Adam(lr=lr, decay = decay)\n        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer,metrics=[\"accuracy\"])    \n        \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-01T02:57:58.781351Z","iopub.execute_input":"2021-06-01T02:57:58.781661Z","iopub.status.idle":"2021-06-01T02:57:58.788865Z","shell.execute_reply.started":"2021-06-01T02:57:58.781632Z","shell.execute_reply":"2021-06-01T02:57:58.788083Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"INIT_LR = 1e-4\nEPOCHS = 30\n\nmodel = create_model(lr=INIT_LR, decay=INIT_LR/EPOCHS,training=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T02:58:05.799457Z","iopub.execute_input":"2021-06-01T02:58:05.799799Z","iopub.status.idle":"2021-06-01T02:58:09.338480Z","shell.execute_reply.started":"2021-06-01T02:58:05.799747Z","shell.execute_reply":"2021-06-01T02:58:09.337641Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n9412608/9406464 [==============================] - 0s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 64\n\nmy_checkpointer = [\n                EarlyStopping(monitor='val_loss', patience=5, verbose=0),\n                ModelCheckpoint(filepath=\"License_character_recognition.h5\", verbose=1, save_weights_only=True)\n                ]\n\nresult = model.fit(image_gen.flow(trainX, trainY, batch_size=BATCH_SIZE), \n                   steps_per_epoch=len(trainX) // BATCH_SIZE, \n                   validation_data=(testX, testY), \n                   validation_steps=len(testX) // BATCH_SIZE, \n                   epochs=EPOCHS, callbacks=my_checkpointer)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T02:58:20.777171Z","iopub.execute_input":"2021-06-01T02:58:20.777510Z","iopub.status.idle":"2021-06-01T03:19:53.953626Z","shell.execute_reply.started":"2021-06-01T02:58:20.777477Z","shell.execute_reply":"2021-06-01T03:19:53.952737Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/30\n529/529 [==============================] - 68s 113ms/step - loss: 2.4683 - accuracy: 0.3695 - val_loss: 1.7340 - val_accuracy: 0.5921\n\nEpoch 00001: saving model to License_character_recognition.h5\nEpoch 2/30\n529/529 [==============================] - 58s 110ms/step - loss: 0.4652 - accuracy: 0.8759 - val_loss: 0.4600 - val_accuracy: 0.8788\n\nEpoch 00002: saving model to License_character_recognition.h5\nEpoch 3/30\n529/529 [==============================] - 58s 110ms/step - loss: 0.3175 - accuracy: 0.9146 - val_loss: 0.2459 - val_accuracy: 0.9357\n\nEpoch 00003: saving model to License_character_recognition.h5\nEpoch 4/30\n529/529 [==============================] - 59s 111ms/step - loss: 0.2492 - accuracy: 0.9309 - val_loss: 0.2043 - val_accuracy: 0.9391\n\nEpoch 00004: saving model to License_character_recognition.h5\nEpoch 5/30\n529/529 [==============================] - 58s 109ms/step - loss: 0.1930 - accuracy: 0.9452 - val_loss: 0.1785 - val_accuracy: 0.9522\n\nEpoch 00005: saving model to License_character_recognition.h5\nEpoch 6/30\n529/529 [==============================] - 58s 110ms/step - loss: 0.1958 - accuracy: 0.9432 - val_loss: 0.1630 - val_accuracy: 0.9538\n\nEpoch 00006: saving model to License_character_recognition.h5\nEpoch 7/30\n529/529 [==============================] - 58s 110ms/step - loss: 0.1471 - accuracy: 0.9563 - val_loss: 0.1369 - val_accuracy: 0.9577\n\nEpoch 00007: saving model to License_character_recognition.h5\nEpoch 8/30\n529/529 [==============================] - 59s 111ms/step - loss: 0.1397 - accuracy: 0.9592 - val_loss: 0.1164 - val_accuracy: 0.9686\n\nEpoch 00008: saving model to License_character_recognition.h5\nEpoch 9/30\n529/529 [==============================] - 58s 110ms/step - loss: 0.1309 - accuracy: 0.9610 - val_loss: 0.1369 - val_accuracy: 0.9583\n\nEpoch 00009: saving model to License_character_recognition.h5\nEpoch 10/30\n529/529 [==============================] - 58s 109ms/step - loss: 0.1215 - accuracy: 0.9639 - val_loss: 0.1084 - val_accuracy: 0.9663\n\nEpoch 00010: saving model to License_character_recognition.h5\nEpoch 11/30\n529/529 [==============================] - 57s 108ms/step - loss: 0.1094 - accuracy: 0.9672 - val_loss: 0.0864 - val_accuracy: 0.9724\n\nEpoch 00011: saving model to License_character_recognition.h5\nEpoch 12/30\n529/529 [==============================] - 58s 109ms/step - loss: 0.1029 - accuracy: 0.9670 - val_loss: 0.1059 - val_accuracy: 0.9692\n\nEpoch 00012: saving model to License_character_recognition.h5\nEpoch 13/30\n529/529 [==============================] - 58s 110ms/step - loss: 0.1004 - accuracy: 0.9686 - val_loss: 0.0934 - val_accuracy: 0.9705\n\nEpoch 00013: saving model to License_character_recognition.h5\nEpoch 14/30\n529/529 [==============================] - 58s 110ms/step - loss: 0.0879 - accuracy: 0.9729 - val_loss: 0.0967 - val_accuracy: 0.9702\n\nEpoch 00014: saving model to License_character_recognition.h5\nEpoch 15/30\n529/529 [==============================] - 58s 109ms/step - loss: 0.0990 - accuracy: 0.9697 - val_loss: 0.1089 - val_accuracy: 0.9689\n\nEpoch 00015: saving model to License_character_recognition.h5\nEpoch 16/30\n529/529 [==============================] - 58s 109ms/step - loss: 0.0909 - accuracy: 0.9719 - val_loss: 0.0756 - val_accuracy: 0.9771\n\nEpoch 00016: saving model to License_character_recognition.h5\nEpoch 17/30\n529/529 [==============================] - 58s 110ms/step - loss: 0.0775 - accuracy: 0.9749 - val_loss: 0.0705 - val_accuracy: 0.9777\n\nEpoch 00017: saving model to License_character_recognition.h5\nEpoch 18/30\n529/529 [==============================] - 58s 110ms/step - loss: 0.0778 - accuracy: 0.9756 - val_loss: 0.0784 - val_accuracy: 0.9748\n\nEpoch 00018: saving model to License_character_recognition.h5\nEpoch 19/30\n529/529 [==============================] - 58s 109ms/step - loss: 0.0720 - accuracy: 0.9781 - val_loss: 0.0740 - val_accuracy: 0.9750\n\nEpoch 00019: saving model to License_character_recognition.h5\nEpoch 20/30\n529/529 [==============================] - 58s 109ms/step - loss: 0.0675 - accuracy: 0.9795 - val_loss: 0.0733 - val_accuracy: 0.9753\n\nEpoch 00020: saving model to License_character_recognition.h5\nEpoch 21/30\n529/529 [==============================] - 57s 109ms/step - loss: 0.0773 - accuracy: 0.9754 - val_loss: 0.0848 - val_accuracy: 0.9769\n\nEpoch 00021: saving model to License_character_recognition.h5\nEpoch 22/30\n529/529 [==============================] - 58s 109ms/step - loss: 0.0707 - accuracy: 0.9782 - val_loss: 0.0988 - val_accuracy: 0.9726\n\nEpoch 00022: saving model to License_character_recognition.h5\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# save model architectur as json file\nmodel_json = model.to_json()\nwith open(\"MobileNets_character_recognition.json\", \"w\") as json_file:\n    json_file.write(model_json)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T03:19:53.955475Z","iopub.execute_input":"2021-06-01T03:19:53.955779Z","iopub.status.idle":"2021-06-01T03:19:54.010607Z","shell.execute_reply.started":"2021-06-01T03:19:53.955736Z","shell.execute_reply":"2021-06-01T03:19:54.009849Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}